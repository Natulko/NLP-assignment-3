{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89603959",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment 3: The Notebook\n",
    "\n",
    "This is the notebook for the third and final hand-in assignment for Natural Language Processing. The notebook counts for 100% of the total assignment, the total assignment counts towards 20% of the final grade.\n",
    "\n",
    "In this notebook, you will be using the Huggingface Transformers library to work with pretrained transformer-based language models. Our running task will be: Natural Language Inference.\n",
    "\n",
    "The assignment broadly consists of three parts:\n",
    "\n",
    "1. Data preparation: where you will learn about the task, and prepare the data to be consumed by your PyTorch model.\n",
    "2. Model finetuning: where you finetune a `transformers` model on the task.\n",
    "3. Multilingual comparison: where you will compare the results on the English dataset to results on its Dutch incarnation.\n",
    "4. In-context learning: where you see how well a non-finetuned generative model like GPT-2 works on the same task.\n",
    "\n",
    "\n",
    "### Note \n",
    "When finetuning huggingface models, the models are saved to your computer. These files can be big (500MB-1GB), so do not hand them in! Instead, make sure that all cell outputs after running the code are visible (so: not cleared) when you hand in the assignment, this way we can see that you've done the training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e0149",
   "metadata": {},
   "source": [
    "## Part 1 (14 points): Data preparation\n",
    "\n",
    "In this part you will familiarize yourself with the task at hand: Natural Language Inference. Recall from the course lectures that Natural Language Inference is a three-way sequence classification task over two sentences. Given a premise and a hypothesis, the task is to decide whether the premise Entails, Contradicts, or is Neutral with respect to the hypothesis. We will work with the SICK (Sentences Involving Compositional Knowledge) dataset of (Marelli et al. 2014) and its Dutch incarnation (Wijnholds & Moortgat, 2021).\n",
    "\n",
    "But first, we need to ensure that we have all the right packages installed, and then make some initial package imports, as usual. We assume that by now you have `torch` already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Transformers library ([torch] is used to get the correct version of accelerate)\n",
    "!pip install transformers[torch]\n",
    "# HuggingFace Datasets library\n",
    "!pip install datasets\n",
    "# HuggingFace Evaluate library\n",
    "!pip install evaluate\n",
    "# Scikit Learn, for evaluation metrics and confusion matrix\n",
    "!pip install scikit-learn\n",
    "# Seaborn, for nice plots\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e290b",
   "metadata": {},
   "source": [
    "### The SICK Dataset\n",
    "\n",
    "The SICK dataset was introduced in 2014 as one of the first dataset to measure relatedness between full sentences, but additionally also is labelled with Natural Language Inference labels. The good news for us is that the Dutch version of SICK, the SICK-NL dataset, is actually on the HuggingFace Hub: ['maximedb/sick_nl'](https://huggingface.co/datasets/maximedb/sick_nl). You can go ahead and check out some samples through the link, or check out the code below; loading the data is now incredibly simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('maximedb/sick_nl')\n",
    "display(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2203d1",
   "metadata": {},
   "source": [
    "Isn't that sick? The example above shows the structure of the data, containing Dutch and English premise (`sentence_A`) and hypothesis (`sentence_B`) sentences. You may notice that `entailment_label` and `label`; the latter is just the integer version of the actual label.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfffb3",
   "metadata": {},
   "source": [
    "### Part 1.1 (6 points):\n",
    "\n",
    "Check out some more examples of the train data, until you find out the correspondence between entailment labels and the integer versions of them. Prove it by finishing the implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'Contradiction': NotImplemented, 'Neutral': NotImplemented, 'Entailment': NotImplemented}\n",
    "id2label = {NotImplemented: 'Contradiction', NotImplemented: 'Neutral', NotImplemented: 'Entailment'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fac00",
   "metadata": {},
   "source": [
    "### Part 1.2 (8 points): Tokenization\n",
    "\n",
    "Now, as we've seen in the lectures and in the HuggingFace NLP course, we need to prepare the raw data in a form that `transformers` models will understand. Again, unlike the previous hand-in assignment, preparing the data is very simple. The below code loads a tokenizer for a BERT base model (uncased), and creates a tokenized version of the data, and prepares a data collator, which we will need to use to wrap everything up properly during training.\n",
    "\n",
    "The only missing part is the implementation of `tokenize_function` below. It takes in a data point (like the example printed above) and returns a tokenized model input, ready to pass to a BERT model. Finish its implementation, ensuring that it returns the correct tokenized input (refer to the slides if you need to recall how you tokenize a pair of sentences for BERT). You can run the test code underneath to verify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7c43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(tokenizer, example):\n",
    "    return NotImplemented\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(bert_tokenizer, x), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input IDs:')\n",
    "print(tokenized_datasets['train'][0]['input_ids'])\n",
    "print('\\nToken Type IDs:')\n",
    "print(tokenized_datasets['train'][0]['token_type_ids'])\n",
    "print('\\nAttention Mask:')\n",
    "print(tokenized_datasets['train'][0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc4c84",
   "metadata": {},
   "source": [
    "## Part 2 (30 points): Finetuning BERT\n",
    "\n",
    "So far so good. Now it's time to finetune a BERT model! Don't worry, the dataset was chosen to be small enough for you to finetune on your own machine (and on CPU), if you have 8GB+ of working memory available. Okay let's get to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab48ac",
   "metadata": {},
   "source": [
    "### Part 2.1 (6 Points)\n",
    "\n",
    "Given that we have the name of the BERT model we want to train, we need to load in a pretrained model. Finish the one-liner below to setup a model for three-way classification so you can finetune for Natural Language Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "bert_model = NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9de0d",
   "metadata": {},
   "source": [
    "Wow, a one-liner to define a whole model! Let's continue with the model training.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2bb58",
   "metadata": {},
   "source": [
    "### Part 2.2 (12 Points)\n",
    "\n",
    "Let's get to training. Again, HuggingFace provides us with a lot of built-in functionality. The code below sets everything up: `compute_metrics` implements the method for calculating accuracy during training, using the `evaluate` library. Then, we have to set up a `Trainer` with a number of `TrainingArguments`. Finish the implementation so that we will run for 3 epochs, with a training batch size low enough for your machine (on the test machine, an M1 MacBook Air 2020 with 16GB working memory, a batch size of 16 was used). Check what device the implementation is going to use (CPU, CUDA, MPS?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# If you're running on an M1/M2 MacBook, with MPS backend support,\n",
    "# you can replace \"TrainingArguments\" by \"TrainingArgumentsWithMPSSupport\"\n",
    "# If not, just ignore this Python class!\n",
    "class TrainingArgumentsWithMPSSupport(TrainingArguments):\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "        \n",
    "training_args = TrainingArguments(\"my-trainer\",\n",
    "                                  per_device_train_batch_size=NotImplemented,\n",
    "                                  num_train_epochs=NotImplemented,\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  dataloader_num_workers=0,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_total_limit=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    bert_model,\n",
    "    training_args,\n",
    "    train_dataset=NotImplemented,\n",
    "    eval_dataset=NotImplemented,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "display(training_args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d78e92",
   "metadata": {},
   "source": [
    "Now, press the button on the cell below, and make some tea while you wait for the finetuning to finish :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d255e0f",
   "metadata": {},
   "source": [
    "Finally, now use the `Trainer` (that already loaded the best performing checkpoint/epoch), to evaluate on the test set and display test accuracy. It should be above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2b Solution:\n",
    "test_predictions = NotImplemented\n",
    "print('Test accuracy: ', test_predictions[2]['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586adfea",
   "metadata": {},
   "source": [
    "### Part 2.3 (12 Points)\n",
    "\n",
    "Wasn't that incredibly easy? However, we would like to have a bit more insight in the model's predictions now. For this, we are going to look into precision, recall, and F1 score for the different classes.\n",
    "First, complete the implementation below to retrieve, for the test set, the model's predicted labels and the correct labels. Then inspect the confusion matrix that comes out, and its pretty-printed heatmap version.\n",
    "Finally, the precision, recall and f1 score are also printed. Use those to explain the confusion matrix: are the model's predictions at the rows and the correct answers at the columns or the other way around?\n",
    "\n",
    "*If you're confused about what a confusion matrix is, check out [Scikit Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) or review the slides of Week 3 (the part on multiclass evaluation and micro/macro-averaging).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "preds = NotImplemented\n",
    "trues = NotImplemented\n",
    "\n",
    "cf_matrix = confusion_matrix(trues, preds)\n",
    "display(cf_matrix)\n",
    "\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(trues, preds)\n",
    "display(precision)\n",
    "display(recall)\n",
    "display(f1)\n",
    "display(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07092925",
   "metadata": {},
   "source": [
    "## Part 3 (24 points): Multilingual comparison\n",
    "\n",
    "Hey, this dataset you're using... it contains English and Dutch! In fact, let's revisit an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2ee8e",
   "metadata": {},
   "source": [
    "In fact, the items in the dataset are all *aligned*. That is, each `sentence_A` is a Dutch translation of `sentence_A_original`, and each `sentence_B` is a translation of `sentence_B_original`. That means we could also finetune a Dutch BERT model on the same dataset! Your task for this part is to do exactly this, and then compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812fda7d",
   "metadata": {},
   "source": [
    "### Part 3.1 (12 points)\n",
    "\n",
    "In this part, your task is quite simple: repeat the finetuning exactly as before, but now use:\n",
    "\n",
    "- (a) the Dutch sentences instead of the English ones\n",
    "- (b) a Dutch tokenizer and BERT model, as indicated below\n",
    "\n",
    "In the end, report the test set accuracy, and the other evaluation metrics (precision, recall, F1) exactly as above, again plotting the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a24003",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_bert_name = 'GroNLP/bert-base-dutch-cased'\n",
    "nl_tokenizer = NotImplemented\n",
    "nl_model = NotImplemented\n",
    "NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1620ee",
   "metadata": {},
   "source": [
    "### Part 3.2 (12 points)\n",
    "Now we wish to quantify the difference between the Dutch and English model results. Execute the following:\n",
    "\n",
    "1. Gather those predictions and true labels for which the English and Dutch model disagree, quantifying the percentage of cases where they disagree.\n",
    "2. Then, calculate and display the English confusion matrix, and Dutch confusion matrix for these cases.\n",
    "3. Then report on your findings. For example, when the models disagree, does the English model have a stronger tendency to classify sentence pairs as Neutral?\n",
    "\n",
    "*Note: the heatmap plots are in separate cells to avoid Seaborn to plot them on top of each other :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_trues, dis_en_preds, dis_nl_preds = NotImplemented\n",
    "\n",
    "dis_en_cf_matrix = confusion_matrix(dis_trues, dis_en_preds) \n",
    "dis_nl_cf_matrix = confusion_matrix(dis_trues, dis_nl_preds) \n",
    "sns.heatmap(dis_en_cf_matrix, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dis_nl_cf_matrix, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7d06e-a41a-46fb-a8dc-236e9554c6e2",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "[Answer for 3. here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7440dcd",
   "metadata": {},
   "source": [
    "## Part 4 (32 points): In-context learning\n",
    "\n",
    "Okay, while it's great that we can reach high accuracy with low effort by using built-in functionality from HuggingFace, let's try and see if we can do without fine-tuning at all, and use *in-context learning* with a generative model on the exact same task.\n",
    "\n",
    "Recall that for in-context learning, we take a large pretrained generative model (like GPT-3) and pose it with a prompt that specifies our task format and then we hope it generates text for a new case that corresponds to the correct answer! In this way we can do classification as well.\n",
    "\n",
    "Now for the bad news: OpenAI never officially released their GPT-3+ models, so we will do with the last available version, GPT-2*. No worries though: since this model is so much smaller we can actually use it on our own machines ;-) in the end, you just need to change the model's name to try out a larger model as soon as you get your hands on a powerful enough computing device.\n",
    "\n",
    "Let's start with setting up the GPT-2 model in the right setting: text generation. We will make use of HuggingFace's built-in `pipeline` for this. Start by running the below code that sets up the generative model in text generation mode.\n",
    "\n",
    "**In fact, the version of GPT-2 we'll be using is not even the largest GPT-2 model around. But hey, you get the general idea right? Just swapping a model's name will allow us to perform the exact same task, just with a larger model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa1613",
   "metadata": {},
   "source": [
    "Now let's see how generation works with some starting prompt. Note that we're setting a seed to guarantee that the model will give the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(5287935)\n",
    "\n",
    "prompt = \"My incomplete sentence is\"\n",
    "sequences_full = pipe(prompt, max_new_tokens=15, return_full_text=True)\n",
    "sequences_generated = pipe(prompt, max_new_tokens=15, return_full_text=False)\n",
    "print(sequences_full[0]['generated_text'])\n",
    "print('\\n\\n')\n",
    "print(sequences_generated[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7786d",
   "metadata": {},
   "source": [
    "Okay, let's try a serious NLI prompt. We will try a few-shot setting in which the model will have seen one example for each NLI label. The code below grabs one example for each label from the validation data, and places them in a prompt, which contains a test pair (Does \"This is difficult.\" entail \"This is easy.\"?). Then, we ask the model to generate the next few tokens to see if it gives some sensible prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf131347",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_example = raw_datasets['validation'][0]\n",
    "co_example_sentence_a = co_example['sentence_A_original']\n",
    "co_example_sentence_b = co_example['sentence_B_original']\n",
    "\n",
    "ne_example = raw_datasets['validation'][1]\n",
    "ne_example_sentence_a = ne_example['sentence_A_original']\n",
    "ne_example_sentence_b = ne_example['sentence_B_original']\n",
    "\n",
    "en_example = raw_datasets['validation'][7]\n",
    "en_example_sentence_a = en_example['sentence_A_original']\n",
    "en_example_sentence_b = en_example['sentence_B_original']\n",
    "\n",
    "prompt = f\"For Sentence A and Sentence B, classify as Entailment, Neutral, or Contradiction.\\n\\\n",
    "Sentence A: {co_example_sentence_a}\\nSentence B: {co_example_sentence_b}\\nNLI Label: Contradiction.\\n\\\n",
    "Sentence A: {ne_example_sentence_a}\\nSentence B: {ne_example_sentence_b}\\nNLI Label: Neutral.\\n\\\n",
    "Sentence A: {en_example_sentence_a}\\nSentence B: {en_example_sentence_b}\\nNLI Label: Entailment.\\n\\\n",
    "Sentence A: This is difficult.\\nSentence B: This is easy.\\n NLI Label:\"\n",
    "\n",
    "prompting_examples = pipe(prompt, max_new_tokens=3, return_full_text=False)\n",
    "print(prompting_examples[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ee3d5",
   "metadata": {},
   "source": [
    "Pretty cool, right? Is the answer correct?\n",
    "\n",
    "If we want to systematically assess how well the model does on the full dataset, we will need a few ingredients, and these are the steps you will follow:\n",
    "\n",
    "1. A way to encode each sentence pair as a prompt.\n",
    "2. A loop to run the model on all of the prompts.\n",
    "3. Functionality to decode the model's output back to an NLI label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0e889",
   "metadata": {},
   "source": [
    "### Part 4.1 (10 points)\n",
    "\n",
    "First off, implement the function `create_prompt` below, that returns an NLI prompt but with the two given sentences (A and B). Verify with the code underneath to see what happens.\n",
    "\n",
    "*Hint: you can re-use the prompt above in your solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f941efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sentence_a, sentence_b):\n",
    "    NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = raw_datasets['test'][0]\n",
    "prompt = create_prompt(ex['sentence_A_original'], ex['sentence_B_original'])\n",
    "print(prompt)\n",
    "sequences = pipe(prompt, max_new_tokens=3, return_full_text=False)\n",
    "display(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333132f",
   "metadata": {},
   "source": [
    "### Part 4.2 (10 points)\n",
    "\n",
    "You may notice that the output is not exactly clean, and it could even be a completely different text than an NLI label! So you'll need to finish the function `decode_prompting_result` below, that will take the output of the generation and return an actual label. Note that the function should return the correct label if the output corresponds to an NLI label, and a fourth label in case the output is something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prompting_result(result: str) -> int:\n",
    "    result_label = NotImplemented\n",
    "    if result_label in label2id:\n",
    "        return label2int[result_label]\n",
    "    else:\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e2986",
   "metadata": {},
   "source": [
    "### Part 4.3 (5 points)\n",
    "\n",
    "Now to actually run the whole thing: for each item in the test data we want to create a prompt, feed it to the model, and transform the result into a label. We want to end up with a list of predictions, just like with the finetuned models before. The only difference will be that we have a fourth possible label. Run the below code as is (don't forget to make tea while you wait!), and run the code underneath to see a sample of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generation_preds = []\n",
    "for d in tqdm(raw_datasets['test']):\n",
    "    prompt = create_prompt(d['sentence_A_original'], d['sentence_B_original'])\n",
    "    results = pipe(prompt, max_new_tokens=3, return_full_text=False)\n",
    "    label = decode_prompting_result(results[0]['generated_text'])\n",
    "    generation_preds.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation_preds[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfe0b1",
   "metadata": {},
   "source": [
    "### Part 4.4 (7 points)\n",
    "\n",
    "As a last step, do what you do best and display the confusion matrix, precision, recall and F1 score for the prompting setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a54a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your 4.4 Solution:\n",
    "\n",
    "NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a2abe",
   "metadata": {},
   "source": [
    "### Bonus Exercise: Decoding strategies\n",
    "\n",
    "If you are unsatisfied with the result, you may be happy to know that you can apply the decoding strategies you saw in the lecture (such as beam search, top-k sampling, top-p nucleus sampling) also in the current context, by adding the same arguments to the `pipe` when you run it on a prompt. You will get bonus points for trying out at least two different generation strategies and seeing how this affects the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8de56f",
   "metadata": {},
   "source": [
    "### Alternative Bonus Exercise: Seed-averaging\n",
    "\n",
    "You may notice that text generation can be different on the same prompt each time that you run the model. You can score bonus points by running the model over the dataset three times and aggregating the results in a way you choose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
